---
title: Data Generation Overview
last_modified_at: 2018-06-06
---

"The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data." - John Tukey  

"If you torture the data, it will confess to anything" - Ronald Coase.

Upfront consideration to the design and execution of data generation can help to prevent unfortunate or costly outcomes.  This section provides guidance on study design, human subjects research, management of clinical and experimental data, and a review of factors to consider when choosing from the common next-generation sequencing or array based techniques.  

## Study Design
In this section we will provide some guidance for researchers looking to develop a hypothesis that will have reasonable statistical power, identify the appropriate set of samples, and execute a large scale data production from those samples.  
There are the two general types of studies using large scale molecular data sets, which can loosely be thought of as "investigative" and "comparative."  The two aren't completely extricable and can each can serve as groundwork for future experiments for the other.  The process to perform these types of studies, however, can be very different.  The details specific to each study type are best addressed prior to generating materials or data sets.  

Topics:
* Proposal Materials and Support
* Hypothesis Development and Big Molecular Data
* Statistics

## IRB and Human Subjects
Topics:
* Consenting and Big Molecular Data
* Privacy and Security
* De-identification
* Data Sharing and Public Repositories

## Clinical and Experimental Data
For a each study, the particular covariates associated with large scale data sets typically come from clinical or laboratory data. When these data are originating from human samples, certain protections need to be in place to ensure patient privacy.  There are resources at the Fred Hutch which can help researchers effectively manage these data so that they can be associated with downstream molecular data sets more consistently and securely.  

The data dictionary being used in the Translational Genomics Data Commons can be [viewed here](https://translationalgenomics.fredhutch.org/annotations.html){:target="_blank"}<!--_--> as it is expanded and developed to reflect the contributions of more participants.  We have developed a data management system that uses modules that are discussed in the associated pages and can facilitate data management via collaboration.  Email Amy Paguirigan to set up an evaluation to see if our system can meet your needs.  

Topics:
* Clinical Covariates
* Specimen Banking  
* Experimental Covariates

## Large-Scale Data Generation
The decisions required when generating large scale data sets themselves are informed by an understanding of the specimen cohort, any limitations imposed by the consent of the patients from which those specimens were obtained, and the specific hypothesis the researcher is intending to address.  The following pages describe important considerations that range from the particular specimen and nucleic acid isolation to quality control approaches to the platforms available at the Fred Hutch Genomics Shared Resource.  Additionally, there are some specific considerations tailored to either RNA or DNA based genomic data approaches.  While these pages are currently focused on nucleic acid based technologies, we hope to add more in the future regarding other data types generated from other assay materials or using other large scale molecular technologies such as proteomics or metabolomics, etc.  

Topics:
* Assay Material Prep and QC
* DNA-Based Approaches
* RNA-Based Approaches
* Genomics Platforms Available

## Laboratory Management Resources
Topics:
* Support Software
* Shared Equipment and Wisdom
