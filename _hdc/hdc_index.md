---
title: Overview
hdc: True
---

The Hutch Data Core is a Shared Resource at the Fred Hutchinson Cancer Research Center
which works to support the needs of researchers who utilize large amounts of data in
their research. We have a particular focus on the bioinformatic analysis of large-scale
datasets generated by modern instrumentation such as genome sequencing, mass spectrometry,
high-throughput imaging, electron microscopy, etc.
While the needs of researchers vary widely by technical domain, scientific goals, and
computational complexity, the Data Core works to provide support via:

- Instructor-led trainings & self-paced learning
- Project-oriented consultation (via the Bioinformatics Core)
- Data portals for automated analysis and visualization
- Curated bioinformatics workflows for common tasks in genomics, imaging, proteomics, etc.


# Trainings

STUB: Summary paragraph on trainings offered by the Data Core

[Link to information on trainings](/hdc/hdc_training)

# Bioinformatics Services

The process of analyzing datasets generated for a particular experiment or project can be complex, often requiring deep expertise in the technology used to generate the raw data as well as the computational tools needed to process them. The Bioinformatics Core provides researchers with support for this analysis, engaging on the basis of specific projects.

To connect with the Bioinformatics Core, please visit [their webpage](https://www.fredhutch.org/en/research/shared-resources/core-facilities/genomics-bioinformatics/bioinformatics-services.html) or reach out directly to Matthew Fitzgibbon [by email](mailto:mfitzgib@fredhutch.org).

# Data Portals

With recent advancements in modern technology for the analysis and visualization of complex datasets, it has become possible to connect researchers directly with their data using interactive webpages referred to as Data Portals. The Data Core is actively developing a set of data portals designed to provide scientific insight across a variety of research areas. These include resources for the high-performance visualization of large-scale datasets (such as single-cell sequencing) as "data atlases"; an interactive platform for executing bioinformatics workflows in the cloud ("PubWeb"); and a flexible system for rendering and sharing data dashboards inside virtual containers ("Carousel").

For more information, please view the [Data Portal Resources](/hdc/hdc_portals).

# Workflows

The bioinformatic process of analyzing large datasets often requires a series of computational steps, each of which may require a different set of software dependencies. To help coordinate and streamline the execution of these workflows, researchers around the world have started to adopt a set of software tools for workflow management, such as Nextflow, Cromwell, and Snakemake. One of the ways in which the Data Core works to provide support for bioinformatic analysis is by helping to put these workflow management tools directly into the hands of Fred Hutch researchers. This includes assistance with running computational workflows on different computational resources (individual computers, the on-premise HPC cluster, or in the “cloud”), curation of pre-existing workflows for commonly used tasks, and assistance with the development of novel workflows to implement new scientific ideas.

For more information, please view the [Workflow Resources](/hdc/hdc_workflows).
