---
title: Data Ingestion and Public Datasets Best Practices
last_modified_at: 2018-06-06
---
Large scale datasets for a study can come from multiple sources, such as those generated by an outside sequencing center or the Fred Hutch Genomics Shared Resource.  Additionally, a study might rely on publicly available datasets in a repository.  Regardless of the source, if processing is required, data will need to be accessible via a Fred Hutch managed data storage, compute resource, or workstation.  Below, we will outline some of the different approaches that exist to ingest and store only the most relevant portions of the data.

## Data Ingestion for Externally Generated Data
For data made by non-Fred Hutch entities that you would like to transfer to Fred Hutch managed storage for further analysis, there is a multi-step process that Scientific Computing can assist you with.  An issue that arises with large scale biomedical datasets is that the possibility of data corruption and failure to completely transfer is far larger than for smaller data sets. Additionally, the issue of what are the "raw" data, how large are they and what "processed" or "intermediate" files might also be critical to ingest if they are not immediately able to be regenerated by a bioinformatic process by a Fred Hutch investigator.  Thus, it is important to work with Scientific Computing to ensure that data to be transferred are ALL of the correct data, are not corrupted or incomplete, as well as transferred to a location suitable for its intended use to balance cost, accessibility, security (especially with human specimen-originating datasets) as well as address backup/archive service requirements.  

The process is generally to:
- Fred Hutch user downloads data from an external source to the *Scratch* service (using ftp, scp, ascp, etc).

OR

- Provide the sequencing center or data source the information needed to copy the data into one of the Fred Hutch Managed Amazon S3 transfer buckets.

THEN
- Then validate the md5 checksums of the data against the checksum info (usually a text file containing md5sums) provided by the sequencing center or data source. (This checks for data corruption or incomplete transfers)

- Transfer the validated data to the PI's *Economy File* service either locally or to the *Economy Cloud* service

### Available Resources
  - For consulting about how to handle large amounts of externally or internally generated data email FH username **scicomp**.
  - For assistance with Fred Hutch users manually coordinating and initiating a transfer of large amounts of data to *Scratch*, email helpdesk to get set up using [Aspera.](https://aspera.fhcrc.org/index.html){:target="_blank"}<!--_-->

## Data Locations for Internally Generated Data
For data made by Fred Hutch researchers via the Genomics Shared Resource, the default data deposition is currently managed directly by Genomics, and will result in the data being made available to the researchers via their *Fast File* directory: /fh/fast/lastname_f/SR/ngs for sequencing data.  Other types of datasets are transferred to researchers in either a dnaarray directory or via other forms of transfer specific to the platform type.  This allows for rapid access to recently generated datasets.  However, once data generated via the Genomics Core becomes of primary interest to archive for occasional use, it is a good idea to visit the Data Storage section and consider implementing the active data management scheme described above with the assistance of Scientific Computing.  

For example, depending on the intended use of the datasets, it may be desirable once data is generated by the Genomics Shared Resource to archive the data to the researcher's *Economy* storage space, with a copy put in *Scratch* or researcher AWS S3 bucket for immediate processing.  The specific organization of archive and working copies of data will depend on the particular project involved.  

### Available Resources
  - For consulting about how to handle large amounts of externally or internally generated data email FH username **scicomp**.
  - For additional assistance regarding data generated via the Fred Hutch Genomics Shared Resource, email FH username **bioinformatics**.

## Publicly Available datasets
There are multiple sources of tiers of data available publicly.  In order to avoid, for example, a researcher having to pay to host large, raw datasets that are publicly available, there are approaches to accessing and documenting only the minimum required data.  Knowing how best to approach publicly available large scale data sets can both make a study far more productive in a shorter period of time and save resources from being unnecessarily spent on generating new data or storing copies of existing datasets when not required for the particular research they are being used for.  This section will have more to come on this topic.  

### Available resources
  - [cBioPortal](http://www.cbioportal.org/){:target="_blank"}<!--_--> is an excellent web-accessible resource to query various publicly available study data from projects such as TCGA or other more specific studies.  
  - [Sage Bionetwork's Synapse platform](https://www.synapse.org/#!StandaloneWiki:OpenResearchProjects){:target="_blank"}<!--_--> hosts and organizes several open research projects that involve large scale molecular data sets and researchers can follow their documentation to download data through the web or python clients.  
